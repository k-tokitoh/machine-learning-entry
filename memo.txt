強化学習は教師となるデータを試行によって得るってことかな。

仕事で行動指標からユーザーをクラスタリングすることもできるかもしれない。
事例としても購買行動に基づくセグメンテーションが挙げられている。

なるほど。訓練データでいくつもモデルをつくって、検証データを食わせてモデルを評価してハイパーパラメータを決める。
最終的に採用されたモデルにテストデータを与えて性能評価とする。なぜ検証データとテストデータをわけるのだろうか？

あー。ハイパーパラメータの最適値を探るために検証データに一番当てはまりがいいやつを選ぶということをする。
あるデータに対して過学習というか、そのデータに対してたまたまあてはまりがいいというのを完全に除くことはできません、受け入れましょう。
ただし検証データに対して精度90%がでたとしても、それはたまたま検証データへの当てはまりがいいだけかもしれないので、最終的にこれくらいの汎用的な精度です、というときには学習にも検証にもつかってないデータをつかうべしだよね、と。

pandasってpanel-data-sらしい。panel dataは表敬式のデータのこと。
scikit-learnはレコメンドや強化学習、生成はスコープ外。

機械学習 > neural network > deep learning

jupyterはもともとipython: interactive pythonだったが、julia, python, Rと広がったので頭文字をとってjupyterになった。

回帰でも変数のスケールは結果に影響するので正規化する（0-1にする）

正則化とは過学習を抑制するテクニック。重みパラメータに制約をかける。

過学習は一部のパラメータの絶対値が大きくなり過ぎて起こることが多いので、大きくなりすぎないように抑制する。

最小化する対象として、誤差の二乗和にペナルティを加えることでこれを実現する。

L1正則化(lasso)では一部の特徴量はつかわれない。
そこがL1/L2を使い分けるポイント。

numpyのadvanced indexingはブラケットの中にmaskを指定するやつだと思われる。

ロジスティック回帰の尤度関数の部分はスキップする。

分類課題はとりあえずロジスティック回帰 or SVCを試す。
イメージとして、ロジスティック回帰は全体をみて境界を決定する、SVCは境界付近をみて境界を決定する。

誤分類がないのがハードマージンSVC, 誤分類があるのがソフトマージンSVC
svm.LinearSVC() をつかえばよしなに適当な方にしてくれる。

ソフトマージンでは、最大化する対象として、逆側の要素へのマージンの負数をペナルティとして加える。
ペナルティの重み付けはハイパーパラメータとして与える。

kernel SVC もあるが、ランダムフォレストやその派生である勾配ブースティング決定木がつかわれがち。
GBDT: gradient boosting decision tree

マージンの最大化は、実際にはマージンの逆数の最小化で求められる。
最小化の方が解きやすいから。
hinge関数は入力が負数なら0を、入力が正数なら入力と等しい値を返す。

決定木の良さは解釈性が高いところ。
決定木は特徴量同士のスケールが異なっていても問題ないので正規化不要。






